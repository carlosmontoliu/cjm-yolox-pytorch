{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss\n",
    "\n",
    "> An implementation of the loss function for training the [YOLOX](https://arxiv.org/abs/2107.08430) object detection model based on [OpenMMLab](https://github.com/open-mmlab)â€™s implementation in the [mmdetection](https://github.com/open-mmlab/mmdetection) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, Type, List, Optional, Callable, Tuple, Union\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cjm_yolox_pytorch.utils import multi_apply, generate_grid_priors\n",
    "from cjm_yolox_pytorch.simota import SimOTAAssigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SamplingResult:\n",
    "    \"\"\"\n",
    "    Bounding box sampling result.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/core/bbox/samplers/sampling_result.py#L7)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 positive_indices:np.array, # Indices of the positive samples.\n",
    "                 negative_indices:np.array, # Indices of the negative samples.\n",
    "                 bboxes:np.array, # Array containing all bounding boxes.\n",
    "                 ground_truth_bboxes:torch.Tensor, # Tensor containing all ground truth bounding boxes.\n",
    "                 assignment_result, # Object that contains the ground truth indices and labels corresponding to each sample.\n",
    "                 ground_truth_flags:np.array # Array indicating which samples are ground truth.\n",
    "                ):\n",
    "        # Indices of positive and negative samples\n",
    "        self.positive_indices = positive_indices\n",
    "        self.negative_indices = negative_indices\n",
    "\n",
    "        # Bounding boxes for positive and negative samples\n",
    "        self.positive_bboxes = bboxes[positive_indices]\n",
    "        self.negative_bboxes = bboxes[negative_indices]\n",
    "        \n",
    "        # Flag indicating if the positive samples are ground truth\n",
    "        self.is_positive_ground_truth = ground_truth_flags[positive_indices]\n",
    "\n",
    "        # Number of ground truths\n",
    "        self.number_of_ground_truths = ground_truth_bboxes.shape[0]\n",
    "        \n",
    "        # Indices of the assigned ground truths for positive samples\n",
    "        self.positive_assigned_ground_truth_indices = assignment_result.ground_truth_box_indices[positive_indices] - 1\n",
    "\n",
    "        # Check the consistency of ground truth bounding boxes and assigned indices\n",
    "        if ground_truth_bboxes.numel() == 0:\n",
    "            if self.positive_assigned_ground_truth_indices.numel() != 0:\n",
    "                raise ValueError('Mismatch between ground truth bounding boxes and positive assigned ground truth indices.')\n",
    "            self.positive_ground_truth_bboxes = torch.empty_like(ground_truth_bboxes).view(-1, 4)\n",
    "        else:\n",
    "            if len(ground_truth_bboxes.shape) < 2:\n",
    "                ground_truth_bboxes = ground_truth_bboxes.view(-1, 4)\n",
    "            self.positive_ground_truth_bboxes = ground_truth_bboxes[self.positive_assigned_ground_truth_indices, :]\n",
    "\n",
    "        # If labels are assigned, assign labels for positive samples. Otherwise, set it as None\n",
    "        if assignment_result.category_labels is not None:\n",
    "            self.positive_ground_truth_labels = assignment_result.category_labels[positive_indices]\n",
    "        else:\n",
    "            self.positive_ground_truth_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOXLoss:\n",
    "    \"\"\"\n",
    "    YOLOXLoss class implements the loss function used in the YOLOX model. \n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/dense_heads/yolox_head.py#L321)\n",
    "\n",
    "    #### Pseudocode\n",
    "    1. Generate the grid of prior boxes by calling the `generate_grid_priors` function with arguments based on the shape of class scores and strides.\n",
    "    2. Update the centroids of the prior boxes based on their widths and heights.\n",
    "    3. Flatten the predicted class scores, bounding box predictions, and objectness scores across all scales and concatenate them. This is done by calling the `flatten_and_concat` method for each of these predictions.\n",
    "    4. Decode the predicted bounding boxes by calling the `bbox_decode` method, which calculates the actual coordinates of the predicted boxes based on the prior boxes and the predicted bounding box transformations.\n",
    "    5. For each image in the batch, compute the targets for classification, bounding box regression, objectness, and optionally, L1 regression. This is done by calling the `get_target_single` method, which matches prior boxes to ground truth boxes, samples positive and negative boxes, and then computes the targets. \n",
    "    6. Calculate the total number of positive samples across all images in the batch.\n",
    "    7. Concatenate all the computed targets across all images.\n",
    "    8. If the `use_l1` flag is set to `True`, concatenate the computed L1 targets across all images.\n",
    "    9. Compute the bounding box, objectness, and class losses by comparing the predictions with the targets. \n",
    "    10. Multiply the computed losses by their respective weights.\n",
    "    11. Create a dictionary to store the computed losses.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes:int, # The number of target classes.\n",
    "                 bbox_loss_weight:float=5.0, # The weight for the loss function to calculate the bounding box regression loss.\n",
    "                 class_loss_weight:float=1.0, # The weight for the loss function to calculate the classification loss.\n",
    "                 objectness_loss_weight:float=1.0, # The weight for the loss function to calculate the objectness loss.\n",
    "                 l1_loss_weight:float=1.0, # The weight for the loss function to calculate the L1 loss.\n",
    "                 use_l1:bool=False, # Whether to use L1 loss in the calculation.\n",
    "                 strides:List[int]=[8,16,32] # The list of strides.\n",
    "                ):\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        giou_loss_partial = partial(torchvision.ops.generalized_box_iou_loss, reduction='none', eps=1e-16)\n",
    "        self.bbox_loss_func = lambda bx1, bx2 : (1-(1-giou_loss_partial(boxes1=bx1, boxes2=bx2))**2).sum()\n",
    "        self.class_loss_func = partial(F.binary_cross_entropy_with_logits, reduction='sum')\n",
    "        self.objectness_loss_func = partial(F.binary_cross_entropy_with_logits, reduction='sum')\n",
    "        self.l1_loss_func = partial(F.l1_loss, reduction='sum')\n",
    "        \n",
    "        self.bbox_loss_weight = bbox_loss_weight\n",
    "        self.class_loss_weight = class_loss_weight\n",
    "        self.objectness_loss_weight = objectness_loss_weight\n",
    "        self.l1_loss_weight = l1_loss_weight\n",
    "        \n",
    "        self.use_l1 = use_l1\n",
    "        \n",
    "        # Initialize the assigner\n",
    "        self.assigner = SimOTAAssigner(center_radius=2.5)\n",
    "        \n",
    "        self.strides = strides\n",
    "        \n",
    "        \n",
    "    def bbox_decode(self, prior_boxes, predicted_boxes):\n",
    "        \"\"\"\n",
    "        Decodes the predicted bounding boxes based on the prior boxes.\n",
    "\n",
    "        Args:\n",
    "            prior_boxes (torch.Tensor): The prior bounding boxes.\n",
    "            predicted_boxes (torch.Tensor): The predicted bounding boxes.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The decoded bounding boxes.\n",
    "        \"\"\"\n",
    "        # Calculate box centroids (geometric centers) and sizes\n",
    "        box_centroids = (predicted_boxes[..., :2] * prior_boxes[:, 2:]) + prior_boxes[:, :2]\n",
    "        box_sizes = torch.exp(predicted_boxes[..., 2:]) * prior_boxes[:, 2:]\n",
    "\n",
    "        # Calculate corners of bounding boxes\n",
    "        top_left = box_centroids - box_sizes / 2\n",
    "        bottom_right = box_centroids + box_sizes / 2\n",
    "\n",
    "        # Stack coordinates to create decoded bounding boxes\n",
    "        decoded_boxes = torch.cat((top_left, bottom_right), -1)\n",
    "\n",
    "        return decoded_boxes\n",
    "\n",
    "\n",
    "    def sample(self, assignment_result, bboxes, ground_truth_boxes, **kwargs):\n",
    "        \"\"\"\n",
    "        Samples positive and negative indices based on the assignment result.\n",
    "        \n",
    "        Args:\n",
    "            assignment_result (Object): The assignment result obtained from assigner.\n",
    "            bboxes (torch.Tensor): The predicted bounding boxes.\n",
    "            ground_truth_boxes (torch.Tensor): The ground truth boxes.\n",
    "\n",
    "        Returns:\n",
    "            SamplingResult: The sampling result containing positive and negative indices.\n",
    "        \"\"\"\n",
    "        positive_indices = torch.nonzero(\n",
    "            assignment_result.ground_truth_box_indices > 0, as_tuple=False).squeeze(-1).unique()\n",
    "        negative_indices = torch.nonzero(\n",
    "            assignment_result.ground_truth_box_indices == 0, as_tuple=False).squeeze(-1).unique()\n",
    "        ground_truth_flags = bboxes.new_zeros(bboxes.shape[0], dtype=torch.uint8)\n",
    "        sampling_result = SamplingResult(positive_indices, negative_indices, bboxes, ground_truth_boxes,\n",
    "                                         assignment_result, ground_truth_flags)\n",
    "        return sampling_result\n",
    "\n",
    "\n",
    "    def get_l1_target(self, l1_target, ground_truth_boxes, prior_boxes, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Calculates the L1 target.\n",
    "\n",
    "        Args:\n",
    "            l1_target (torch.Tensor): The L1 target tensor.\n",
    "            ground_truth_boxes (torch.Tensor): The ground truth boxes.\n",
    "            prior_boxes (torch.Tensor): The prior bounding boxes.\n",
    "            epsilon (float, optional): A small value to prevent division by zero. Defaults to 1e-8.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The updated L1 target.\n",
    "        \"\"\"\n",
    "        ground_truth_centroid_and_wh = torchvision.ops.box_convert(ground_truth_boxes, 'xyxy', 'cxcywh')\n",
    "        l1_target[:, :2] = (ground_truth_centroid_and_wh[:, :2] - prior_boxes[:, :2]) / prior_boxes[:, 2:]\n",
    "        l1_target[:, 2:] = torch.log(ground_truth_centroid_and_wh[:, 2:] / prior_boxes[:, 2:] + epsilon)\n",
    "        return l1_target\n",
    "\n",
    "\n",
    "    def get_target_single(self, class_preds, objectness_score, prior_boxes, decoded_bboxes,\n",
    "                           ground_truth_bboxes, ground_truth_labels):\n",
    "        \"\"\"\n",
    "        Calculates the targets for a single image.\n",
    "\n",
    "        Args:\n",
    "            class_preds (torch.Tensor): The predicted class probabilities.\n",
    "            objectness_score (torch.Tensor): The predicted objectness scores.\n",
    "            prior_boxes (torch.Tensor): The prior bounding boxes.\n",
    "            decoded_bboxes (torch.Tensor): The decoded bounding boxes.\n",
    "            ground_truth_bboxes (torch.Tensor): The ground truth boxes.\n",
    "            ground_truth_labels (torch.Tensor): The ground truth labels.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: The targets for classification, objectness, bounding boxes, and L1 (if applicable), along with\n",
    "            the foreground mask and the number of positive samples.\n",
    "        \"\"\"\n",
    "        # Get the number of prior boxes and ground truth labels\n",
    "        num_prior_boxes = prior_boxes.size(0)\n",
    "        num_ground_truths = ground_truth_labels.size(0)\n",
    "\n",
    "        # Match dtype of ground truth bounding boxes to the dtype of decoded bounding boxes\n",
    "        ground_truth_bboxes = ground_truth_bboxes.to(decoded_bboxes.dtype)\n",
    "\n",
    "        # Check if there are no ground truth labels (objects) in the image\n",
    "        if num_ground_truths == 0:\n",
    "            # Initialize targets as zero tensors, and foreground_mask as a boolean tensor with False values\n",
    "            class_targets = class_preds.new_zeros((0, self.num_classes))\n",
    "            bbox_targets = class_preds.new_zeros((0, 4))\n",
    "            l1_targets = class_preds.new_zeros((0, 4))\n",
    "            objectness_targets = class_preds.new_zeros((num_prior_boxes, 1))\n",
    "            foreground_mask = class_preds.new_zeros(num_prior_boxes).bool()\n",
    "            return (foreground_mask, class_targets, objectness_targets, bbox_targets,\n",
    "                    l1_targets, 0)  # Return zero for num_positive_per_image\n",
    "\n",
    "        # Calculate the offset for the prior boxes\n",
    "        offset_prior_boxes = torch.cat([prior_boxes[:, :2] + prior_boxes[:, 2:] * 0.5, prior_boxes[:, 2:]], dim=-1)\n",
    "\n",
    "        # Assign ground truth objects to prior boxes and get assignment results\n",
    "        assignment_result = self.assigner.assign(\n",
    "            class_preds.sigmoid() * objectness_score.unsqueeze(1).sigmoid(),\n",
    "            offset_prior_boxes, decoded_bboxes, ground_truth_bboxes, ground_truth_labels)\n",
    "\n",
    "        # Use assignment results to sample prior boxes\n",
    "        sampling_result = self.sample(assignment_result, prior_boxes, ground_truth_bboxes)\n",
    "        \n",
    "        # Get the indices of positive (object-containing) samples\n",
    "        positive_indices = sampling_result.positive_indices\n",
    "        num_positive_per_image = positive_indices.size(0)\n",
    "\n",
    "        # Get the maximum IoU values for the positive samples\n",
    "        positive_ious = assignment_result.max_iou_values[positive_indices]\n",
    "\n",
    "        # Generate class targets\n",
    "        class_targets = F.one_hot(sampling_result.positive_ground_truth_labels, self.num_classes) * positive_ious.unsqueeze(-1)\n",
    "\n",
    "        # Initialize objectness targets as zeros and set the values at positive_indices to 1\n",
    "        objectness_targets = torch.zeros_like(objectness_score).unsqueeze(-1)\n",
    "        objectness_targets[positive_indices] = 1\n",
    "\n",
    "        # Generate bounding box targets\n",
    "        bbox_targets = sampling_result.positive_ground_truth_bboxes\n",
    "\n",
    "        # Initialize L1 targets as zeros\n",
    "        l1_targets = class_preds.new_zeros((num_positive_per_image, 4))\n",
    "\n",
    "        # If use_l1 is True, calculate L1 targets\n",
    "        if self.use_l1:\n",
    "            l1_targets = self.get_l1_target(l1_targets, bbox_targets, prior_boxes[positive_indices])\n",
    "\n",
    "        # Initialize foreground_mask as zeros and set the values at positive_indices to True\n",
    "        foreground_mask = torch.zeros_like(objectness_score).to(torch.bool)\n",
    "        foreground_mask[positive_indices] = 1\n",
    "\n",
    "        # Return the computed targets, the foreground mask, and the number of positive samples per image\n",
    "        return (foreground_mask, class_targets, objectness_targets, bbox_targets, l1_targets, num_positive_per_image)\n",
    "    \n",
    "    \n",
    "    def flatten_and_concat(self, tensors, num_images, reshape_dims=None):\n",
    "        new_shape = (num_images, -1, reshape_dims) if reshape_dims else (num_images, -1)\n",
    "        return torch.cat([t.permute(0, 2, 3, 1).reshape(*new_shape) for t in tensors], dim=1)\n",
    "\n",
    "    \n",
    "    def __call__(self, num_images, class_scores, predicted_bboxes, objectness_scores, ground_truth_bboxes, ground_truth_labels):\n",
    "        \"\"\"\n",
    "        Main method to compute the YOLOX loss.\n",
    "\n",
    "        Args:\n",
    "            num_images (int): The number of images in a batch.\n",
    "            class_scores (List[torch.Tensor]): A list of class scores for each scale.\n",
    "            predicted_bboxes (List[torch.Tensor]): A list of predicted bounding boxes for each scale.\n",
    "            objectness_scores (List[torch.Tensor]): A list of objectness scores for each scale.\n",
    "            ground_truth_bboxes (List[torch.Tensor]): A list of ground truth bounding boxes for each image.\n",
    "            ground_truth_labels (List[torch.Tensor]): A list of ground truth labels for each image.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary with the classification, bounding box, objectness, and optionally, L1 loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate prior box coordinates for all anchors.\n",
    "        grid_priors = generate_grid_priors(*[s*self.strides[0] for s in class_scores[0].shape[-2:]], self.strides)\n",
    "        grid_priors[:, :2] *= grid_priors[:, 2].unsqueeze(1)\n",
    "        flatten_prior_boxes = torch.cat([grid_priors, grid_priors[:, 2:].clone()], dim=1)\n",
    "        \n",
    "        # Flatten and concatenate class predictions, bounding box predictions, and objectness scores\n",
    "        flatten_class_preds = self.flatten_and_concat(class_scores, num_images, self.num_classes)\n",
    "        flatten_bbox_preds = self.flatten_and_concat(predicted_bboxes, num_images, 4)\n",
    "        flatten_objectness_scores = self.flatten_and_concat(objectness_scores, num_images)\n",
    "                    \n",
    "        # Concatenate and decode box predictions\n",
    "        flatten_prior_boxes = flatten_prior_boxes.to(flatten_bbox_preds.device)\n",
    "        flatten_decoded_bboxes = self.bbox_decode(flatten_prior_boxes, flatten_bbox_preds)\n",
    "\n",
    "        # Compute targets\n",
    "        (positive_masks, class_targets, objectness_targets, bbox_targets, l1_targets,\n",
    "         num_positive_images) = multi_apply(\n",
    "             self.get_target_single, flatten_class_preds.detach(),\n",
    "             flatten_objectness_scores.detach(),\n",
    "             flatten_prior_boxes.unsqueeze(0).repeat(num_images, 1, 1),\n",
    "             flatten_decoded_bboxes.detach(), ground_truth_bboxes, ground_truth_labels)\n",
    "\n",
    "        # Calculate total number of samples\n",
    "        num_total_samples = max(sum(num_positive_images), 1)\n",
    "\n",
    "        # Concatenate all positive masks, class targets, objectness targets, and bounding box targets\n",
    "        positive_masks = torch.cat(positive_masks, 0)\n",
    "        class_targets = torch.cat(class_targets, 0)\n",
    "        objectness_targets = torch.cat(objectness_targets, 0)\n",
    "        bbox_targets = torch.cat(bbox_targets, 0)\n",
    "\n",
    "        # Compute bounding box loss\n",
    "        loss_bbox = self.bbox_loss_func(flatten_decoded_bboxes.view(-1, 4)[positive_masks], bbox_targets)\n",
    "\n",
    "        # Compute objectness loss\n",
    "        loss_obj = self.objectness_loss_func(flatten_objectness_scores.view(-1, 1), objectness_targets)\n",
    "\n",
    "        # Compute class loss\n",
    "        loss_cls = self.class_loss_func(flatten_class_preds.view(-1, self.num_classes)[positive_masks],class_targets)\n",
    "                \n",
    "        # Scale losses\n",
    "        loss_bbox = (loss_bbox * self.bbox_loss_weight) / num_total_samples\n",
    "        loss_obj = (loss_obj * self.objectness_loss_weight) / num_total_samples\n",
    "        loss_cls = (loss_cls * self.class_loss_weight) / num_total_samples\n",
    "        \n",
    "        # Initialize loss dictionary\n",
    "        loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_obj=loss_obj)\n",
    "\n",
    "        # If use_l1 is True, concatenate l1 targets, compute L1 loss and add it to the loss dictionary\n",
    "        if self.use_l1:\n",
    "            l1_targets = torch.cat(l1_targets, 0)\n",
    "            loss_l1 = self.l1_loss_func(\n",
    "                flatten_bbox_preds.view(-1, 4)[positive_masks],\n",
    "                l1_targets) / num_total_samples\n",
    "            loss_l1 *= self.l1_loss_weight\n",
    "            loss_dict.update(loss_l1=loss_l1)\n",
    "\n",
    "        # Return loss dictionary\n",
    "        return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
