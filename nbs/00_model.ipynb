{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> A PyTorch implementation of the [YOLOX](https://arxiv.org/abs/2107.08430) object detection model based on [OpenMMLab](https://github.com/open-mmlab)â€™s implementation in the [mmdetection](https://github.com/open-mmlab/mmdetection) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from typing import Any, Type, List, Optional, Callable, Tuple\n",
    "from functools import partial\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cjm_yolox_pytorch.utils import multi_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MODEL_TYPES = ['yolox_tiny', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x']\n",
    "\n",
    "CSP_DARKNET_CFGS = {\n",
    "    MODEL_TYPES[0]:dict(deepen_factor=0.33, widen_factor=0.375),\n",
    "    MODEL_TYPES[1]:dict(deepen_factor=0.33, widen_factor=0.5),\n",
    "    MODEL_TYPES[2]:dict(deepen_factor=0.67, widen_factor=0.75),\n",
    "    MODEL_TYPES[3]:dict(deepen_factor=1.0, widen_factor=1.0),\n",
    "    MODEL_TYPES[4]:dict(deepen_factor=1.33, widen_factor=1.25)\n",
    "}\n",
    "\n",
    "PAFPN_CFGS = {\n",
    "    MODEL_TYPES[0]:dict(in_channels=[96, 192, 384], out_channels=96, num_csp_blocks=1),\n",
    "    MODEL_TYPES[1]:dict(in_channels=[128, 256, 512], out_channels=128, num_csp_blocks=1),\n",
    "    MODEL_TYPES[2]:dict(in_channels=[192, 384, 768], out_channels=192, num_csp_blocks=2),\n",
    "    MODEL_TYPES[3]:dict(in_channels=[256, 512, 1024], out_channels=256, num_csp_blocks=3),\n",
    "    MODEL_TYPES[4]:dict(in_channels=[320, 640, 1280], out_channels=320, num_csp_blocks=4),\n",
    "}\n",
    "\n",
    "HEAD_CFGS = {\n",
    "    MODEL_TYPES[0]:dict(in_channels=96,feat_channels=96),\n",
    "    MODEL_TYPES[1]:dict(in_channels=128,feat_channels=128),\n",
    "    MODEL_TYPES[2]:dict(in_channels=192, feat_channels=192),\n",
    "    MODEL_TYPES[3]:dict(in_channels=256, feat_channels=256),\n",
    "    MODEL_TYPES[4]:dict(in_channels=320, feat_channels=320),\n",
    "}\n",
    "\n",
    "OPENMMLAB_CKPT_URL = 'https://download.openmmlab.com/mmdetection/v2.0/yolox'\n",
    "\n",
    "PRETRAINED_URLS = {\n",
    "    MODEL_TYPES[0]:f'{OPENMMLAB_CKPT_URL}/yolox_tiny_8x8_300e_coco/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth',\n",
    "    MODEL_TYPES[1]:f'{OPENMMLAB_CKPT_URL}/yolox_s_8x8_300e_coco/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth',\n",
    "    MODEL_TYPES[2]:None,\n",
    "    MODEL_TYPES[3]:f'{OPENMMLAB_CKPT_URL}/yolox_l_8x8_300e_coco/yolox_l_8x8_300e_coco_20211126_140236-d3bd2b23.pth',\n",
    "    MODEL_TYPES[4]:f'{OPENMMLAB_CKPT_URL}/yolox_x_8x8_300e_coco/yolox_x_8x8_300e_coco_20211126_140254-1ef88d67.pth',\n",
    "}\n",
    "\n",
    "NORM_CFG = dict(momentum=0.03, eps=0.001)\n",
    "\n",
    "MODEL_CFGS = {model_type: {**CSP_DARKNET_CFGS[model_type], \n",
    "                            **{'neck_'+k: v for k, v in PAFPN_CFGS[model_type].items()}, \n",
    "                            **{'head_'+k: v for k, v in HEAD_CFGS[model_type].items()}, \n",
    "                            **{k:{\"pretrained\": v != None} for k,v in PRETRAINED_URLS.items()}[model_type]} \n",
    "               for model_type in MODEL_TYPES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolox_tiny'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = MODEL_TYPES[0]\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Configurable block used for Convolution2d-Normalization-Activation blocks.\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input x):\n",
    "    \n",
    "        1. Pass the input (x) through the convolutional layer and store the result back to x.\n",
    "        2. Pass the output from the convolutional layer (now stored in x) through the batch normalization layer and store the result back to x.\n",
    "        3. Apply the activation function to the output of the batch normalization layer (x) and return the result.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels: int,  # Number of channels in the input image\n",
    "                 out_channels: int, # Number of channels produced by the convolution\n",
    "                 kernel_size: int,  # Size of the convolving kernel\n",
    "                 stride: int = 1,   # Stride of the convolution.\n",
    "                 padding: int = 0,  # Zero-padding added to both sides of the input.\n",
    "                 bias: bool = True, # If set to False, the layer will not learn an additive bias.\n",
    "                 eps: float = 1e-05,    # A value added to the denominator for numerical stability in BatchNorm2d.\n",
    "                 momentum: float = 0.1, # The value used for the running_mean and running_var computation in BatchNorm2d.\n",
    "                 affine: bool = True,   # If set to True, this module has learnable affine parameters.\n",
    "                 track_running_stats: bool = True, # If set to True, this module tracks the running mean and variance.\n",
    "                 activation_function: Type[nn.Module] = nn.SiLU # The activation function to be applied after batch normalization.\n",
    "                ):\n",
    "        \n",
    "        super(ConvModule, self).__init__()\n",
    "\n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        # Batch normalization layer\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)\n",
    "        # Activation function\n",
    "        self.activate = activation_function()\n",
    "        \n",
    "        init.kaiming_normal_(self.conv.weight.data, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Pass input through convolutional layer\n",
    "        x = self.conv(x)\n",
    "        # Pass output from convolutional layer through batch normalization\n",
    "        x = self.bn(x)\n",
    "        # Apply activation function and return result\n",
    "        return self.activate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DarknetBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Darknet bottleneck block used in Darknet.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/utils/csp_layer.py#L8)\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input_tensor x):\n",
    "    \n",
    "    1. Store x as identity.\n",
    "    2. Pass x through the first convolutional layer (conv1), and then the result through the second convolutional layer (conv2). Store the output as 'out'.\n",
    "    3. Check if add_identity is True:\n",
    "       a. If True, check if identity_conv exists (i.e., when the input and output channels do not match).\n",
    "          i. If identity_conv exists, pass identity through the identity_conv layer and update the identity.\n",
    "       b. Add the updated identity to the 'out'.\n",
    "    4. Return 'out' as the final output.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int, # The number of input channels to the block.\n",
    "                 out_channels: int, # The number of output channels from the block.\n",
    "                 eps: float = 0.001, # A value added to the denominator for numerical stability in the ConvModule's BatchNorm layer.\n",
    "                 momentum: float = 0.03, # The value used for the running_mean and running_var computation in the ConvModule's BatchNorm layer.\n",
    "                 affine: bool = True, # A flag that when set to True, gives the ConvModule's BatchNorm layer learnable affine parameters.\n",
    "                 track_running_stats: bool = True, # If True, the ConvModule's BatchNorm layer will track the running mean and variance.\n",
    "                 add_identity: bool = True # If True, add an identity shortcut (also known as skip connection) to the output.\n",
    "                ):\n",
    "        super(DarknetBottleneck, self).__init__()\n",
    "\n",
    "        self.add_identity = add_identity\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # First convolutional layer with 1x1 kernel size\n",
    "        self.conv1 = ConvModule(in_channels, out_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "        \n",
    "        # Second convolutional layer with 3x3 kernel size\n",
    "        self.conv2 = ConvModule(out_channels, out_channels, kernel_size=3, stride=1, padding=1, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "        \n",
    "        # Add a 1x1 conv on shortcut when in and out channels don't match\n",
    "        if self.add_identity and self.in_channels != self.out_channels:\n",
    "            self.identity_conv = ConvModule(in_channels, out_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                            bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                            track_running_stats=track_running_stats)\n",
    "        else:\n",
    "            self.identity_conv = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "                \n",
    "        # Store the input tensor as identity tensor for possible use in shortcut connection\n",
    "        identity = x\n",
    "        # Pass the input tensor through two conv layers in sequence\n",
    "        out = self.conv2(self.conv1(x))\n",
    "        \n",
    "        # If add_identity is True, then add the original input (identity) to the output of the conv layers\n",
    "        if self.add_identity:\n",
    "            # Apply a conv layer to the identity tensor if the in and out channels don't match\n",
    "            if self.identity_conv is not None:\n",
    "                identity = self.identity_conv(identity)\n",
    "            # Add the identity tensor to the output tensor\n",
    "            out += identity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CSPLayer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cross Stage Partial Layer\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/utils/csp_layer.py#L75)\n",
    "    \n",
    "    #### Pseudocode\n",
    "    1. Start with input `x`\n",
    "    2. Apply `main_conv` layer on `x` and store the output in `x1`\n",
    "    3. Apply `short_conv` layer on `x` and store the output in `x2`\n",
    "    4. For each `block` in the `blocks` list:\n",
    "        - Pass `x1` through the `block` and store the result back in `x1`\n",
    "    5. Concatenate `x1` and `x2` along dimension 1 to form a new tensor called `out`\n",
    "    6. Pass `out` through the `final_conv` layer and store the result back in `out`\n",
    "    7. Return `out` as the final output of the forward function\n",
    "    \n",
    "    Notes:\n",
    "    \n",
    "    - The `main_conv`, `short_conv`, and `final_conv` are convolution layers which apply a series of transformations (including convolution, batch normalization, and non-linear activation) on the input tensor.\n",
    "    - The `blocks` list contains a number of \"bottleneck\" layers that perform additional transformations on the `x1` tensor. Each bottleneck layer has a specific structure, with two convolution layers and an optional identity shortcut.\n",
    "    - The concatenation operation combines the results of the `main_conv` layers processed through `blocks` and the `short_conv` layer, preparing the tensor for the final transformations in `final_conv`.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, \n",
    "                 in_channels, # Number of input channels.\n",
    "                 out_channels, # Number of output channels.\n",
    "                 num_blocks, # Number of blocks in the bottleneck.\n",
    "                 kernel_size=1, # Size of the convolving kernel.\n",
    "                 stride=1, # Stride of the convolution.\n",
    "                 padding=0, # Zero-padding added to both sides of the input.\n",
    "                 eps=0.001, # A value added to the denominator for numerical stability in the ConvModule's BatchNorm layer.\n",
    "                 momentum=0.03, # The value used for the running_mean and running_var computation in the ConvModule's BatchNorm layer.\n",
    "                 affine=True, # A flag that when set to True, gives the ConvModule's BatchNorm layer learnable affine parameters.\n",
    "                 track_running_stats=True, # Whether or not to track the running mean and variance during training.\n",
    "                 add_identity=True # Whether or not to add an identity shortcut connection if the input and output are the same size.\n",
    "                ):\n",
    "        \n",
    "        super(CSPLayer, self).__init__()\n",
    "\n",
    "        # Define the number of hidden_channels as half the number of out_channels\n",
    "        hidden_channels = out_channels // 2\n",
    "\n",
    "        # Define the main convolution layer\n",
    "        self.main_conv = ConvModule(in_channels, hidden_channels, kernel_size, stride, padding, \n",
    "                                    bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                    track_running_stats=track_running_stats)\n",
    "\n",
    "        # Define the short convolution layer\n",
    "        self.short_conv = ConvModule(in_channels, hidden_channels, kernel_size, stride, padding, \n",
    "                                     bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                     track_running_stats=track_running_stats)\n",
    "\n",
    "        # Define the final convolution layer, which takes in the concatenated output from main_conv \n",
    "        # and short_conv and outputs to the final output layer\n",
    "        self.final_conv = ConvModule(2 * hidden_channels, out_channels, kernel_size, stride, padding, \n",
    "                                     bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                     track_running_stats=track_running_stats)\n",
    "        \n",
    "        # Define a list of blocks using the DarknetBottleneck structure\n",
    "        self.blocks = nn.ModuleList([DarknetBottleneck(hidden_channels, hidden_channels, eps, momentum, affine, track_running_stats, add_identity) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Pass the input through the main_conv layer\n",
    "        x1 = self.main_conv(x)\n",
    "\n",
    "        # Pass the input through the short_conv layer\n",
    "        x2 = self.short_conv(x)\n",
    "\n",
    "        # Pass the output of main_conv through each block in the blocks list\n",
    "        for block in self.blocks:\n",
    "            x1 = block(x1)\n",
    "\n",
    "        # Concatenate the outputs of the blocks and the short_conv along dimension 1\n",
    "        out = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Pass the concatenated output through the final_conv layer\n",
    "        out = self.final_conv(out)\n",
    "\n",
    "        # Returning the final output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Focus(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Focus width and height information into channel space.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/backbones/csp_darknet.py#L14)\n",
    "    \n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input_tensor x):\n",
    "\n",
    "    1. Split the input tensor x into four patches (patch_top_left, patch_top_right, patch_bot_left, patch_bot_right) based on their spatial positions in the last two dimensions of x.\n",
    "    2. Concatenate these four patches along the channel dimension and store the output back into 'x'.\n",
    "    3. Pass 'x' through the convolutional layer (self.conv) and store the result as 'out'.\n",
    "    4. Return 'out' as the final output.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels: int, # Number of input channels.\n",
    "                 out_channels: int, # Number of output channels.\n",
    "                 kernel_size: int = 1, # Size of the convolving kernel.\n",
    "                 stride: int = 1, # Stride of the convolution.\n",
    "                 bias: bool = False, # If set to False, the layer will not learn an additive bias.\n",
    "                 eps: float = 0.001, #  A value added to the denominator for numerical stability in the ConvModule's BatchNorm layer.\n",
    "                 momentum: float = 0.03, # The value used for the running_mean and running_var computation in the ConvModule's BatchNorm layer.\n",
    "                 affine: bool = True, # A flag that when set to True, gives the ConvModule's BatchNorm layer learnable affine parameters.\n",
    "                 track_running_stats: bool = True # Whether or not to track the running mean and variance during training.\n",
    "                ):\n",
    "        \n",
    "        super(Focus, self).__init__()\n",
    "        self.conv = ConvModule(\n",
    "            in_channels * 4,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=bias,\n",
    "            eps=eps,\n",
    "            momentum=momentum,\n",
    "            affine=affine,\n",
    "            track_running_stats=track_running_stats)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "                \n",
    "        # Split the input tensor into 4 patches\n",
    "        patch_top_left = x[..., ::2, ::2]   # Top left patch\n",
    "        patch_top_right = x[..., ::2, 1::2]  # Top right patch\n",
    "        patch_bot_left = x[..., 1::2, ::2]  # Bottom left patch\n",
    "        patch_bot_right = x[..., 1::2, 1::2]  # Bottom right patch\n",
    "        \n",
    "        # Concatenate the patches along the channel dimension\n",
    "        x = torch.cat(\n",
    "            (\n",
    "                patch_top_left,\n",
    "                patch_bot_left,\n",
    "                patch_top_right,\n",
    "                patch_bot_right,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SPPBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial Pyramid Pooling layer used in YOLOv3-SPP\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/backbones/csp_darknet.py#L67)\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input_tensor x):\n",
    "\n",
    "    1. Pass x through the first convolutional layer (conv1) and assign the output back to x.\n",
    "    2. Create an empty list called pooling_results and add x to it.\n",
    "    3. For each pooling layer in the poolings list, do the following:\n",
    "       a. Apply the pooling layer on x and append the output to the pooling_results list.\n",
    "    4. Concatenate all tensors in the pooling_results list along the channel dimension (dimension 1) and assign the output back to x.\n",
    "    5. Pass x through the second convolutional layer (conv2) to combine the features from all pooling layers and project them to the desired number of output channels.\n",
    "    6. Return the final output tensor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels: int, # The number of input channels.\n",
    "                 out_channels: int, # The number of output channels.\n",
    "                 pool_sizes: List[int]=[5, 9, 13], # The sizes of the pooling areas.\n",
    "                 eps: float=0.001, # A value added to the denominator for numerical stability in the BatchNorm layer.\n",
    "                 momentum: float=0.03, #  The value used for the running_mean and running_var computation in the BatchNorm layer.\n",
    "                 affine: bool=True, # A flag that when set to True, gives the BatchNorm layer learnable affine parameters.\n",
    "                 track_running_stats: bool=True # Whether to keep track of running mean and variance in BatchNorm.\n",
    "                ):\n",
    "        \n",
    "        super(SPPBottleneck, self).__init__()\n",
    "        \n",
    "        # Reducing the number of channels by a factor of 2\n",
    "        hidden_channels = in_channels // 2\n",
    "\n",
    "        # Convolution layer to reduce the number of channels\n",
    "        self.conv1 = ConvModule(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "\n",
    "        # List of pooling layers with different window sizes\n",
    "        self.poolings = nn.ModuleList([nn.MaxPool2d(kernel_size=ps, stride=1, padding=ps//2) for ps in pool_sizes])\n",
    "\n",
    "        # Convolution layer to combine the features from the pooling layers and project them to the desired number of output channels\n",
    "        self.conv2 = ConvModule(hidden_channels*(len(pool_sizes)+1), out_channels, kernel_size=1, stride=1, padding=0, \n",
    "                                bias=False, eps=eps, momentum=momentum, affine=affine, \n",
    "                                track_running_stats=track_running_stats)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "                \n",
    "        # Reducing the number of channels by a factor of 2 using the convolution layer\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Applying max pooling with different window sizes and collecting the results\n",
    "        pooling_results = [x]\n",
    "        for pooling in self.poolings:\n",
    "            pooling_results.append(pooling(x))\n",
    "\n",
    "        # Concatenating the results of pooling along the channel dimension\n",
    "        x = torch.cat(pooling_results, dim=1)\n",
    "\n",
    "        # Combining the features and projecting them to the desired number of output channels using the convolution layer\n",
    "        return self.conv2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CSPDarknet(nn.Module):\n",
    "    \"\"\"\n",
    "    CSP-Darknet backbone\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/backbones/csp_darknet.py#L124)\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input_tensor x):\n",
    "\n",
    "    1. Initialize an empty list 'outs' to store intermediate outputs.\n",
    "    2. For each index and layer name in the model's layers:\n",
    "       a. Retrieve the layer corresponding to the current layer name.\n",
    "       b. Pass 'x' through the retrieved layer and update 'x' with the output.\n",
    "       c. If the current index is in the set of output indices:\n",
    "          i. Append 'x' to 'outs'.\n",
    "    3. Convert 'outs' to a tuple and return it as the final output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Architecture settings for P5 and P6\n",
    "    ARCH_SETTINGS = {\n",
    "        'P5': [[64, 128, 3, True, False], [128, 256, 9, True, False],\n",
    "               [256, 512, 9, True, False], [512, 1024, 3, False, True]],\n",
    "        'P6': [[64, 128, 3, True, False], [128, 256, 9, True, False],\n",
    "               [256, 512, 9, True, False], [512, 768, 3, True, False],\n",
    "               [768, 1024, 3, False, True]]\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch='P5', # Architecture configuration, 'P5' or 'P6'.\n",
    "                 deepen_factor=1.0, # Factor to adjust the number of channels in each layer.\n",
    "                 widen_factor=1.0, # Factor to adjust the number of blocks in CSP layer.\n",
    "                 out_indices=(2, 3, 4), # Indices of the stages to output.\n",
    "                 spp_kernal_sizes=(5, 9, 13), # Sizes of the pooling operations in the Spatial Pyramid Pooling.\n",
    "                 momentum=0.03, # Momentum for the moving average in batch normalization.\n",
    "                 eps=0.001 # Epsilon for batch normalization to avoid numerical instability.\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if not set(out_indices).issubset(range(len(self.ARCH_SETTINGS[arch]) + 1)):\n",
    "            raise ValueError(\"out_indices are out of range\")\n",
    "\n",
    "        self.out_indices = out_indices\n",
    "        # Building the initial layer of the model\n",
    "        self.stem = Focus(\n",
    "            3,\n",
    "            int(self.ARCH_SETTINGS[arch][0][0] * widen_factor),\n",
    "            kernel_size=3,\n",
    "            stride=1\n",
    "            )\n",
    "        self.layers = ['stem']\n",
    "        # Building the stages of the model\n",
    "        self._build_stages(arch, deepen_factor, widen_factor, spp_kernal_sizes, momentum, eps)\n",
    "\n",
    "    def _build_stages(self, arch, deepen_factor, widen_factor, spp_kernal_sizes, momentum, eps):\n",
    "        \"\"\"\n",
    "        Build the stages of the CSPDarknet model.\n",
    "\n",
    "        Args:\n",
    "            arch (str): Architecture type, 'P5' or 'P6'.\n",
    "            deepen_factor (float): Factor to adjust the depth of the model.\n",
    "            widen_factor (float): Factor to adjust the width of the model.\n",
    "            spp_kernal_sizes (tuple): Sizes of the pooling operations in the Spatial Pyramid Pooling.\n",
    "            momentum (float): Momentum for the moving average in batch normalization.\n",
    "            eps (float): Epsilon for batch normalization to avoid numerical instability.\n",
    "        \"\"\"\n",
    "\n",
    "        # For each stage configuration in the architecture settings\n",
    "        for i, (in_c, out_c, num_blocks, add_identity, use_spp) in enumerate(self.ARCH_SETTINGS[arch]):\n",
    "            # Adjust the channel size based on the widen factor\n",
    "            in_c, out_c = int(in_c * widen_factor), int(out_c * widen_factor)\n",
    "            # Adjust the number of blocks based on the deepen factor\n",
    "            num_blocks = max(round(num_blocks * deepen_factor), 1)\n",
    "\n",
    "            stage = []\n",
    "\n",
    "            # Append ConvModule for the stage\n",
    "            stage.append(ConvModule(in_c, \n",
    "                                    out_c, 3, \n",
    "                                    stride=2, \n",
    "                                    padding=1, \n",
    "                                    bias=False, \n",
    "                                    eps=eps, \n",
    "                                    momentum=momentum, \n",
    "                                    affine=True, \n",
    "                                    track_running_stats=True))\n",
    "\n",
    "            # If use_spp is True, append a Spatial Pyramid Pooling layer\n",
    "            if use_spp:\n",
    "                stage.append(SPPBottleneck(out_c, out_c, pool_sizes=spp_kernal_sizes))\n",
    "\n",
    "            # Append a Cross Stage Partial layer\n",
    "            stage.append(CSPLayer(out_c, out_c, num_blocks=num_blocks, add_identity=add_identity))\n",
    "            # Add the stage to the model as a sequential layer\n",
    "            self.add_module(f'stage{i + 1}', nn.Sequential(*stage))\n",
    "            self.layers.append(f'stage{i + 1}')\n",
    "\n",
    "    def forward(self, x):\n",
    "                \n",
    "        outs = []\n",
    "        # For each layer in the model\n",
    "        for i, layer_name in enumerate(self.layers):\n",
    "            # Get the layer by its name\n",
    "            layer = getattr(self, layer_name)\n",
    "            # Pass the input through the layer\n",
    "            x = layer(x)\n",
    "            # If the index is in out_indices, append the output to outs\n",
    "            if i in self.out_indices:\n",
    "                outs.append(x)\n",
    "        return tuple(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 32, 32]),\n",
       " torch.Size([1, 192, 16, 16]),\n",
       " torch.Size([1, 384, 8, 8])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp_darknet_cfg = CSP_DARKNET_CFGS[model_type]\n",
    "csp_darknet = CSPDarknet(**csp_darknet_cfg)\n",
    "\n",
    "backbone_inp = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    backbone_out = csp_darknet(backbone_inp)\n",
    "[out.shape for out in backbone_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOXPAFPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Path Aggregation Feature Pyramid Network (PAFPN) used in YOLOX.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/necks/yolox_pafpn.py#L14)\n",
    "    \n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(list inputs):\n",
    "\n",
    "    1. Assert that the length of inputs equals the length of in_channels.\n",
    "    2. Initialize inner_outs with the last feature map in inputs.\n",
    "    3. For each index in in_channels in reverse (exclude first):\n",
    "       a. Set feature_high to the first feature map in inner_outs.\n",
    "       b. Set feature_low to the corresponding input feature map.\n",
    "       c. Reduce the channels of feature_high using the corresponding reduce_layer. Update feature_high in inner_outs.\n",
    "       d. Upsample feature_high to the spatial size of feature_low.\n",
    "       e. Concatenate upsampled feature_high and feature_low, and pass the result through the corresponding CSPLayer. The output is inner_out.\n",
    "       f. Add inner_out to the beginning of inner_outs.\n",
    "    4. Initialize outs with the first feature map from inner_outs.\n",
    "    5. For each index in in_channels (exclude last):\n",
    "       a. Set feature_low to the last feature map in outs.\n",
    "       b. Set feature_high to the corresponding feature map in inner_outs.\n",
    "       c. Downsample feature_low to the spatial size of feature_high.\n",
    "       d. Concatenate downsampled feature_low and feature_high, and pass the result through the corresponding CSPLayer. The output is out.\n",
    "       e. Add out to the end of outs.\n",
    "    6. For each convolutional layer (conv) in out_convs:\n",
    "       a. Apply conv to the corresponding feature map in outs.\n",
    "    7. Return the tuple of feature maps in outs as the final output.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels, # The number of input channels for each level of the feature pyramid.\n",
    "                 out_channels, # The number of output channels for each level of the feature pyramid.\n",
    "                 num_csp_blocks=3, # The number of bottlenecks in each CSPLayer.\n",
    "                 upsample_cfg=dict(scale_factor=2, mode='nearest'), # The configuration for the upsampling operation.\n",
    "                 momentum=0.03, # The momentum for the batch normalization in the ConvModule.\n",
    "                 eps=0.001 # The epsilon for the batch normalization in the ConvModule.\n",
    "                ):\n",
    "        super(YOLOXPAFPN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # build top-down blocks, which includes reduce layers and CSP blocks\n",
    "        self.upsample = nn.Upsample(**upsample_cfg)\n",
    "        self.reduce_layers = nn.ModuleList()\n",
    "        self.top_down_blocks = nn.ModuleList()\n",
    "        for idx in range(len(in_channels) - 1, 0, -1):\n",
    "            # add reducing layers for channel-wise feature reduction\n",
    "            self.reduce_layers.append(\n",
    "                ConvModule(\n",
    "                    in_channels[idx],\n",
    "                    in_channels[idx - 1],\n",
    "                    kernel_size=(1, 1), \n",
    "                    stride=(1, 1),\n",
    "                    padding=0,\n",
    "                    bias=False,\n",
    "                    momentum=momentum,\n",
    "                    eps=eps,\n",
    "                    affine=True, \n",
    "                    track_running_stats=True\n",
    "                ))\n",
    "            # add CSP layers for feature learning\n",
    "            self.top_down_blocks.append(\n",
    "                CSPLayer(\n",
    "                    in_channels[idx - 1] * 2,\n",
    "                    in_channels[idx - 1],\n",
    "                    num_blocks=num_csp_blocks,\n",
    "                    add_identity=False))\n",
    "\n",
    "        # build bottom-up blocks, which includes downsampling layers and CSP blocks\n",
    "        self.downsamples = nn.ModuleList()\n",
    "        self.bottom_up_blocks = nn.ModuleList()\n",
    "        for idx in range(len(in_channels) - 1):\n",
    "            # add downsampling layers for spatial reduction\n",
    "            self.downsamples.append(\n",
    "                ConvModule(\n",
    "                    in_channels[idx],\n",
    "                    in_channels[idx],\n",
    "                    3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    bias=False,\n",
    "                    momentum=momentum,\n",
    "                    eps=eps,\n",
    "                    affine=True, \n",
    "                    track_running_stats=True\n",
    "                ))\n",
    "            # add CSP layers for feature learning\n",
    "            self.bottom_up_blocks.append(\n",
    "                CSPLayer(\n",
    "                    in_channels[idx] * 2,\n",
    "                    in_channels[idx + 1],\n",
    "                    num_blocks=num_csp_blocks,\n",
    "                    add_identity=False))\n",
    "\n",
    "        # build output convolutions for each level\n",
    "        self.out_convs = nn.ModuleList()\n",
    "        for i in range(len(in_channels)):\n",
    "            self.out_convs.append(\n",
    "                ConvModule(\n",
    "                    in_channels[i],\n",
    "                    out_channels,\n",
    "                    1,\n",
    "                    stride=(1, 1),\n",
    "                    padding=0,\n",
    "                    bias=False,\n",
    "                    momentum=momentum,\n",
    "                    eps=eps,\n",
    "                    affine=True, \n",
    "                    track_running_stats=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "\n",
    "        # top-down path\n",
    "        inner_outs = [inputs[-1]]\n",
    "        for idx in range(len(self.in_channels) - 1, 0, -1):\n",
    "            feat_heigh = inner_outs[0]\n",
    "            feat_low = inputs[idx - 1]\n",
    "            # reduce the channel dimension of the higher feature map\n",
    "            feat_heigh = self.reduce_layers[len(self.in_channels) - 1 - idx](feat_heigh)\n",
    "            inner_outs[0] = feat_heigh\n",
    "\n",
    "            # upsample the higher feature map to the same spatial size as the lower one\n",
    "            upsample_feat = self.upsample(feat_heigh)\n",
    "\n",
    "            # concatenate the upsampled higher feature map and the lower one, and feed them into the CSPLayer\n",
    "            inner_out = self.top_down_blocks[len(self.in_channels) - 1 - idx](\n",
    "                torch.cat([upsample_feat, feat_low], 1))\n",
    "            # collect the output feature maps\n",
    "            inner_outs.insert(0, inner_out)\n",
    "\n",
    "        # bottom-up path\n",
    "        outs = [inner_outs[0]]\n",
    "        for idx in range(len(self.in_channels) - 1):\n",
    "            feat_low = outs[-1]\n",
    "            feat_height = inner_outs[idx + 1]\n",
    "            # downsample the lower feature map to the same spatial size as the higher one\n",
    "            downsample_feat = self.downsamples[idx](feat_low)\n",
    "            # concatenate the downsampled lower feature map and the higher one, and feed them into the CSPLayer\n",
    "            out = self.bottom_up_blocks[idx](\n",
    "                torch.cat([downsample_feat, feat_height], 1))\n",
    "            # collect the output feature maps\n",
    "            outs.append(out)\n",
    "\n",
    "        # apply the output convolutions to the feature maps\n",
    "        for idx, conv in enumerate(self.out_convs):\n",
    "            outs[idx] = conv(outs[idx])\n",
    "\n",
    "        return tuple(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 32, 32]),\n",
       " torch.Size([1, 96, 16, 16]),\n",
       " torch.Size([1, 96, 8, 8])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pafpn_cfg = PAFPN_CFGS[model_type]\n",
    "yolox_pafpn = YOLOXPAFPN(**pafpn_cfg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    neck_out = yolox_pafpn(backbone_out)\n",
    "[out.shape for out in neck_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOXHead(nn.Module):\n",
    "    \"\"\"\n",
    "    The head of YOLOX model <https://arxiv.org/abs/2107.08430>, used for bounding box prediction.\n",
    "    \n",
    "    Based on OpenMMLab's implementation in the mmdetection library:\n",
    "    \n",
    "    - [OpenMMLab's Implementation](https://github.com/open-mmlab/mmdetection/blob/d64e719172335fa3d7a757a2a3636bd19e9efb62/mmdet/models/dense_heads/yolox_head.py#L20)\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(feats):\n",
    "\n",
    "    1. For each scale level in feats, perform the following steps:\n",
    "        a. Pass the scale level feature through the classification convolutions (cls_convs) to produce a feature map (cls_feat).\n",
    "        b. Pass the same scale level feature through the regression convolutions (reg_convs) to produce another feature map (reg_feat).\n",
    "        c. Apply the classification predictor convolution (conv_cls) on the classification feature map (cls_feat) to get the classification scores (cls_score).\n",
    "        d. Apply the regression predictor convolution (conv_reg) on the regression feature map (reg_feat) to get bounding box predictions (bbox_pred).\n",
    "        e. Apply the objectness predictor convolution (conv_obj) on the regression feature map (reg_feat) to get objectness scores (objectness).\n",
    "    2. Collect the classification scores, bounding box predictions, and objectness scores for each scale level and store them in a tuple.\n",
    "    3. Return the tuple as the final output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    BBOX_DIM = 4\n",
    "    OBJECTNESS_DIM = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes:int, # The number of target classes.\n",
    "                 in_channels:int, # The number of input channels.\n",
    "                 feat_channels=256, # The number of feature channels.\n",
    "                 stacked_convs=2, # The number of convolution layers to stack.\n",
    "                 strides=[8, 16, 32], # The stride of each scale level in the feature pyramid.\n",
    "                 momentum=0.03, # The momentum for the moving average in batch normalization.\n",
    "                 eps=0.001 # The epsilon to avoid division by zero in batch normalization.\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_out_channels = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.feat_channels = feat_channels\n",
    "        self.stacked_convs = stacked_convs\n",
    "        self.strides = strides\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self._init_layers()  # initialize layers\n",
    "\n",
    "    def _init_layers(self):\n",
    "        \"\"\"\n",
    "        Initialize layers for the head module, includes classification \n",
    "        convolutions, regression convolutions and prediction convolutions \n",
    "        for each scale level.\n",
    "        \"\"\"\n",
    "        self.multi_level_cls_convs = nn.ModuleList()\n",
    "        self.multi_level_reg_convs = nn.ModuleList()\n",
    "        self.multi_level_conv_cls = nn.ModuleList()\n",
    "        self.multi_level_conv_reg = nn.ModuleList()\n",
    "        self.multi_level_conv_obj = nn.ModuleList()\n",
    "        for _ in self.strides:\n",
    "            self.multi_level_cls_convs.append(self._build_stacked_convs())\n",
    "            self.multi_level_reg_convs.append(self._build_stacked_convs())\n",
    "            conv_cls, conv_reg, conv_obj = self._build_predictor()\n",
    "            self.multi_level_conv_cls.append(conv_cls)\n",
    "            self.multi_level_conv_reg.append(conv_reg)\n",
    "            self.multi_level_conv_obj.append(conv_obj)\n",
    "\n",
    "    def _build_stacked_convs(self):\n",
    "        \"\"\"\n",
    "        Build stacked convolution layers.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nn.Sequential\n",
    "            A sequential container of stacked conv layers.\n",
    "        \"\"\"\n",
    "        conv = ConvModule\n",
    "        stacked_convs = []\n",
    "        for i in range(self.stacked_convs):\n",
    "            chn = self.in_channels if i == 0 else self.feat_channels\n",
    "            stacked_convs.append(\n",
    "                conv(\n",
    "                    chn,\n",
    "                    self.feat_channels,\n",
    "                    3,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                    momentum=self.momentum,\n",
    "                    eps=self.eps,\n",
    "                    affine=True, \n",
    "                    track_running_stats=True,\n",
    "                    bias=False))\n",
    "        return nn.Sequential(*stacked_convs)\n",
    "\n",
    "    def _build_predictor(self):\n",
    "        \"\"\"\n",
    "        Build predictor layers for classification, regression, and objectness.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            The classification, regression, and objectness convolutional layers.\n",
    "        \"\"\"\n",
    "        conv_cls = nn.Conv2d(self.feat_channels, self.cls_out_channels, 1)\n",
    "        conv_reg = nn.Conv2d(self.feat_channels, self.BBOX_DIM, 1)\n",
    "        conv_obj = nn.Conv2d(self.feat_channels, self.OBJECTNESS_DIM, 1)\n",
    "        return conv_cls, conv_reg, conv_obj\n",
    "\n",
    "    def forward_single(self, x, cls_convs, reg_convs, conv_cls, conv_reg,\n",
    "                       conv_obj):\n",
    "        \"\"\"\n",
    "        Forward feature of a single scale level.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensor\n",
    "            The input tensor.\n",
    "        cls_convs : nn.Module\n",
    "            The classification convolutions.\n",
    "        reg_convs : nn.Module\n",
    "            The regression convolutions.\n",
    "        conv_cls : nn.Module\n",
    "            The classification predictor convolution.\n",
    "        conv_reg : nn.Module\n",
    "            The regression predictor convolution.\n",
    "        conv_obj : nn.Module\n",
    "            The objectness predictor convolution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            The classification scores, bounding box predictions, and objectness.\n",
    "        \"\"\"\n",
    "        cls_feat = cls_convs(x)\n",
    "        reg_feat = reg_convs(x)\n",
    "\n",
    "        cls_score = conv_cls(cls_feat)\n",
    "        bbox_pred = conv_reg(reg_feat)\n",
    "        objectness = conv_obj(reg_feat)\n",
    "\n",
    "        return cls_score, bbox_pred, objectness\n",
    "\n",
    "    def forward(self, feats):\n",
    "        \"\"\"\n",
    "        Forward pass for the head.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feats : list\n",
    "            A list of multi-level features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            The results of applying the forward_single function to each scale level.\n",
    "        \"\"\"\n",
    "        return multi_apply(self.forward_single, feats,\n",
    "                           self.multi_level_cls_convs,\n",
    "                           self.multi_level_reg_convs,\n",
    "                           self.multi_level_conv_cls,\n",
    "                           self.multi_level_conv_reg,\n",
    "                           self.multi_level_conv_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_scores: [torch.Size([1, 80, 32, 32]), torch.Size([1, 80, 16, 16]), torch.Size([1, 80, 8, 8])]\n",
      "bbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\n",
      "objectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "head_cfg = HEAD_CFGS[model_type]\n",
    "yolox_head = YOLOXHead(num_classes=80, **head_cfg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_scores, bbox_preds, objectness = yolox_head(neck_out)    \n",
    "print(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\n",
    "print(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\n",
    "print(f\"objectness: {[objectness.shape for objectness in objectness]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YOLOX(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of `YOLOX: Exceeding YOLO Series in 2021`\n",
    "    \n",
    "    - <https://arxiv.org/abs/2107.08430>\n",
    "    \n",
    "    #### Pseudocode\n",
    "    Function forward(input_tensor x):\n",
    "\n",
    "    1. Pass x through the backbone module. The backbone module performs feature extraction from the input images. Store the output as 'x'.\n",
    "    2. Pass the updated x through the neck module. The neck module performs feature aggregation of the extracted features. Update 'x' with the new output.\n",
    "    3. Pass the updated x through the bbox_head module. The bbox_head module predicts bounding boxes for potential objects in the images using the aggregated features. Update 'x' with the new output.\n",
    "    4. Return 'x' as the final output. The final 'x' represents the model's predictions for object locations within the input images.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 backbone:CSPDarknet, # Backbone module for feature extraction.\n",
    "                 neck:YOLOXPAFPN, # Neck module for feature aggregation.\n",
    "                 bbox_head:YOLOXHead): # Bbox head module for predicting bounding boxes.\n",
    "        super(YOLOX, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.neck = neck\n",
    "        self.bbox_head = bbox_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through backbone\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # Forward through neck\n",
    "        x = self.neck(x)\n",
    "\n",
    "        # Forward through bbox_head\n",
    "        x = self.bbox_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_scores: [torch.Size([1, 80, 32, 32]), torch.Size([1, 80, 16, 16]), torch.Size([1, 80, 8, 8])]\n",
      "bbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\n",
      "objectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "yolox = YOLOX(csp_darknet, yolox_pafpn, yolox_head)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_scores, bbox_preds, objectness = yolox(backbone_inp)    \n",
    "print(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\n",
    "print(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\n",
    "print(f\"objectness: {[objectness.shape for objectness in objectness]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def kaiming_init(module:torch.nn.Module # The module to be initialized.\n",
    "                ) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the weights of the Conv2d layers of the given model using the [Kaiming Normal initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_).\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the module is a 2d convolutional layer\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        \n",
    "        # Apply Kaiming Normal initialization to the weights of the module\n",
    "        # We use 'fan_out' mode as this preserves the magnitude of the variance of the weights\n",
    "        # in the forward pass. The nonlinearity is set to 'relu' as the network uses ReLU activation functions.\n",
    "        init.kaiming_normal_(module.weight.data, mode='fan_out', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0403,  0.0276, -0.0600],\n",
       "        [ 0.0094,  0.0068,  0.0484],\n",
       "        [-0.0257,  0.0385, -0.0589]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(yolox_head.parameters())[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0403,  0.0276, -0.0600],\n",
       "        [ 0.0094,  0.0068,  0.0484],\n",
       "        [-0.0257,  0.0385, -0.0589]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaiming_init(yolox_head)\n",
    "list(yolox_head.parameters())[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init_head(head: YOLOXHead, # The YOLOX head to be initialized.\n",
    "              num_classes: int # The number of classes in the dataset.\n",
    "             ) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the `YOLOXHead` with appropriate class outputs and convolution layers.\n",
    "\n",
    "    This function configures the output channels in the YOLOX head to match the\n",
    "    number of classes in the dataset. It also initializes multiple level\n",
    "    convolutional layers for each stride in the YOLOX head.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the number of output channels in the head to be equal to the number of classes\n",
    "    head.cls_out_channels = num_classes\n",
    "\n",
    "    # Create a list of 2D convolutional layers, one for each stride in the head.\n",
    "    # Each convolutional layer will have a number of output channels equal to the number of classes\n",
    "    # and a kernel size of 1 (i.e., it will perform a 1x1 convolution).\n",
    "    \n",
    "    conv_layers = [nn.Conv2d(head.feat_channels, head.cls_out_channels, 1) for _ in head.strides]\n",
    "    \n",
    "    for conv in conv_layers:\n",
    "        init.kaiming_normal_(conv.weight.data, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    head.multi_level_conv_cls = nn.ModuleList(conv_layers)\n",
    "    \n",
    "    # Use Kaiming initialization to initialize the weights of the convolutional layers. \n",
    "    head.multi_level_conv_cls.apply(kaiming_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolox_head.multi_level_conv_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x Conv2d(96, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_head(yolox_head, 19)\n",
    "yolox_head.multi_level_conv_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cjm_psl_utils.core import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_model(model_type:str, # Type of the model to be built.\n",
    "                num_classes:int, # Number of classes for the model.\n",
    "                pretrained:bool=True, # Whether to load pretrained weights.\n",
    "                checkpoint_dir:str='./pretrained_checkpoints/' # Directory to store checkpoints.\n",
    "               ) -> YOLOX: # The built YOLOX model.\n",
    "    \"\"\"\n",
    "    Builds a YOLOX model based on the given parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert model_type in MODEL_TYPES, f\"Invalid model_type. Expected one of: {MODEL_TYPES}, but got {model_type}\"\n",
    "\n",
    "    backbone_cfg = CSP_DARKNET_CFGS[model_type]\n",
    "    neck_cfg = PAFPN_CFGS[model_type]\n",
    "    head_cfg = HEAD_CFGS[model_type]\n",
    "    \n",
    "    backbone = CSPDarknet(**backbone_cfg)\n",
    "    neck = YOLOXPAFPN(**neck_cfg)\n",
    "\n",
    "    if pretrained and PRETRAINED_URLS[model_type] == None:\n",
    "        print(\"The selected model type does not have a pretrained checkpoint. Initializing model with untrained weights.\")\n",
    "        pretrained = False\n",
    "    \n",
    "    try:\n",
    "        if pretrained:\n",
    "            url = PRETRAINED_URLS[model_type]\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, Path(url).name)\n",
    "            download_file(url, checkpoint_dir)\n",
    "            \n",
    "            pretrained_ckpt = torch.load(checkpoint_path)['state_dict']\n",
    "            num_pretrained_classes = pretrained_ckpt['bbox_head.multi_level_conv_cls.0.weight'].shape[0]\n",
    "            \n",
    "            head = YOLOXHead(num_classes=num_pretrained_classes, **head_cfg)\n",
    "        else:\n",
    "            head = YOLOXHead(num_classes=num_classes, **head_cfg)\n",
    "        \n",
    "        yolox = YOLOX(backbone, neck, head)\n",
    "        \n",
    "        if pretrained:\n",
    "            yolox.load_state_dict(pretrained_ckpt)\n",
    "            init_head(head, num_classes)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while building the model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    return yolox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./pretrained_checkpoints/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth already exists and overwrite is set to False.\n",
      "cls_scores: [torch.Size([1, 19, 32, 32]), torch.Size([1, 19, 16, 16]), torch.Size([1, 19, 8, 8])]\n",
      "bbox_preds: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 8, 8])]\n",
      "objectness: [torch.Size([1, 1, 32, 32]), torch.Size([1, 1, 16, 16]), torch.Size([1, 1, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "yolox = build_model(model_type, 19, pretrained=True)\n",
    "\n",
    "test_inp = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_scores, bbox_preds, objectness = yolox(test_inp)\n",
    "    \n",
    "print(f\"cls_scores: {[cls_score.shape for cls_score in cls_scores]}\")\n",
    "print(f\"bbox_preds: {[bbox_pred.shape for bbox_pred in bbox_preds]}\")\n",
    "print(f\"objectness: {[objectness.shape for objectness in objectness]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
